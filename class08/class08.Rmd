---
title: "Class 8: Machine Learning"
author: "Serina Huang"
date: "October 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

Our first example with `kmeans()` function.

```{r}
# Example plot to see how Rmarkdown works
plot(1:10, typ="o", cex=2, pch = 20, col="pink")
```

Back to k-means...

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
example <- cbind(xval = tmp, yval = rev(tmp))
plot(example)

```

Use the kmeans() function and print the results.
```{r}
k2 <- kmeans(example, centers = 2, nstart = 20)
k2
```

Q. How many points are in each cluster?
```{r}
k2$size
```

Q. What ‘component’ of your result object details cluster assignment/membership?
```{r}
k2$cluster
table(k2$cluster)
```

Q. What is the cluster center?
```{r}
k2$centers
```

Plot the example colored by the cluster assignment. Label cluster centers as blue points.
```{r}
plot(example, col = k2$cluster)
points(k2$centers, col = "blue", pch = 20, cex = 1.5)
# palette(c("blue", "red"))
```

Q. Repeat for k = 3, which has the lower tot.withinss?
```{r}
k3 <- kmeans(example, centers = 3, nstart = 20)
k3$tot.withinss
k2$tot.withinss
```
k = 3 has a lower tot.withinss. A lower tot.withinss doesn't necessarily mean it's better clustering. In fact, to tell what is the best k, you should make a scree plot.

```{r}
k1 <- kmeans(example, centers = 1, nstart = 20)
k4 <- kmeans(example, centers = 4, nstart = 20)
k5 <- kmeans(example, centers = 5, nstart = 20)
k6 <- kmeans(example, centers = 6, nstart = 20)

ss <- c(k1$tot.withinss, k2$tot.withinss, k3$tot.withinss, k4$tot.withinss, k5$tot.withinss, k6$tot.withinss)
plot(ss, typ = "b", xlab = "Number of clusters (k)", ylab = "Total within SS", main = "Scree plot")
```

From the scree plot, we can see that the "elbow" falls at 2 clusters, meaning 2 is the optimal number of clusters for our dataset.

## Hierarchical clustering
First, create a distance matrix by calculating the Euclidean distance between all the points. This will tell you how similar each point is to another point.
```{r}
dist_matrix <- dist(example)
```

The hclust() function returns a hierarchical clustering model. Let's see what the output is.
```{r}
hc <- hclust(dist_matrix)
hc
```

The output isn't too useful, so let's see if we can extract more information about the distance matrix.
```{r}
# What is the dimension of the distance matrix?
dim(dist_matrix)
# Turn dist_matrix into a matrix by filling in the blanks
head( as.matrix(dist_matrix), n = 2L )
# Compare the dimension of the original example dataset vs. dimension of new distance matrix
dim(example)
dim( as.matrix(dist_matrix) )
```

Now we know what's in the distance matrix, we can plot the hierarchical clustering model as a dendrogram.
```{r}
plot(hc)
```

This makes sense. Our original example vector consists of 30 points centered around -3, and the next 30 points centered around 3. Note: the number labels on the x-axis is not the value of the point, but the index.

We see that at height around 5.5, 2 clusters emerge. We can draw a straight line on the plot to show this.
```{r}
plot(hc)
abline(h = 5.5, col = "red")
```

That's useful. However, if you can't tell where the height is visually, or if you want to know the height at which 10 clusters emerge, you will need the cutree function.
```{r}
# Where should I cut to get 3 clusters (groups)?
three_groups <- cutree(hc, k = 3)
# Visualize the clusters made by hierarchical clustering and cutree
plot(example, col = three_groups)
# Where should I cut to get a height of 8?
two_groups <- cutree(hc, h = 8)
# Visualize this too. We should see 2 clusters
plot(example, col = two_groups)
```

Note: the output of cutree yields a vector indicating membership.

Now let's try different hierarchical clustering methods.
```{r}
hc.complete <- hclust(dist_matrix, method = "complete")
plot(hc.complete, main= "Complete Linking Method")

hc.average <- hclust(dist_matrix, method = "average")
plot(hc.average, main = "Average Linking Method")
```

### What if our data are measured on different scales?
This is usually reality. We can normalize each data such that its mean = 0 and standard deviation = 1.

#### Step 1. Generate example data for clustering
```{r}
xc <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), rnorm(50, mean = 0, sd = 0.3)), ncol = 2) # c3
)

colnames(xc) <- c("x", "y")
```

#### Step 2. Plot the data without clustering
```{r}
plot(xc)
```

#### Step 3. Generate colors for known clusters
```{r}
# c1 c1 c1 c1 * 50 times ... c2 c2 c2 c2 ... c3 c3 c3 c3 ...
rep_colvec <- rep( c("c1", "c2", "c3"), each = 50)
reality_colvec <- as.factor(rep_colvec)
plot(example, col = reality_colvec, pch = 20)
```

This looks wrong since we know there should be 3 clusters when we created each vector c1, c2, and c3. We will probably get a more accurate clustering if we normalize each vector.

Q. Use the `dist()`, `hclust()`, `plot()` and `cutree()` to return 2 to 3 clusters.
```{r}
# Create a distance matrix for the created vector xc
d_xc <- dist(xc)
hc_xc <- hclust(d_xc)
plot(hc_xc)
grp2 <- cutree(hc_xc, k = 2)
plot(hc_xc, col = grp2, main = "Two Clusters", pch = 20)
grp3 <- cutree(hc_xc, k = 3)
plot(hc_xc, col = grp3, main = "Three Clusters", pch = 20)
# Compare membership of the different groups obtained from hierarchical clustering
table(grp2)
table(grp3)
```

## Principal component analysis
Let's make an example dataset.
```{r}
# Initialize a blank 100 x 10 matrix
mydata <- matrix(nrow=100, ncol=10)

# Label the rows gene1, gene2, ... gene100
rownames(mydata) <- paste("gene", 1:100, sep="")

# Label the first five columns wt1, wt2, wt3, wt4, wt5 and the last five ko1, ko2, ko3, ko4, ko5
colnames(mydata) <- c( paste("wt", 1:5, sep=""), paste("ko", 1:5, sep="") )

# Fill in fake read counts with Poisson distribution
for(i in 1:nrow(mydata)) {
 wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
 ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
 mydata[i,] <- c(wt.values, ko.values)
}

# Let's check out our data!
head(mydata) 
```

We see that the genes are in rows and the samples are in columns. The `prcomp()` function operates differently, so we need to transform our matrix such that the samples are in rows and the genes are in columns.

```{r}
trans_mydata <- t(mydata)
head(trans_mydata)
```

Now, we are ready for PCA!
```{r}
# In general, scaling is advisable such that all variables have unit variance
pca <- prcomp(trans_mydata, scale = TRUE)
# Since we have 10 samples, we have 10 PCs
attributes(pca)
pca$x
```
As you can tell, PC1 shows a lot of variance among the ten samples, whereas PC10 shows almost no variance.

Let's make a PCA plot with PC1 and PC2!
```{r}
# First, calculate how much variance is captured per PC
pca_var <- pca$sdev ^ 2
# Convert into percent variance and round to first decimal place
per_var <- round( (pca_var / sum(pca_var) ) * 100, 1)
# Make color vector by searching for sample matches and converting sample names to colors
pca_colvec <- colnames(mydata)
pca_colvec[grep("wt", pca_colvec)] <- "red"
pca_colvec[grep("ko", pca_colvec)] <- "blue"
# Plot with colors and labels
plot( pca$x[,1], pca$x[,2],
      xlab = paste0("PC1 (", per_var[1], "%)"),
      ylab = paste0("PC2 (", per_var[2], "%)"),
      col = pca_colvec)
# Label some points and ESC when done
identify(pca$x[,1], pca$x[,2], labels = colnames(mydata))
```

This means that PC1 captured `r per_var[1]`% of the variance in this dataset. We can also make a scree plot to see how much variance other PCs captured.
```{r}
barplot(per_var, main = "Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```

We can also see which genes have the largest effect on PC1, the most important PC. In other words, what are the genes driving the split/structure seen in PC1?
```{r}
loading_scores <- pca$rotation[,1] 
summary(loading_scores)
# Since we are interested in the magnitude of the contribution of each gene, take the absolute value
gene_scores <- abs(loading_scores)
# Now, sort the gene scores from high to low
gene_scores_ranked <- sort(gene_scores, decreasing = TRUE)
# Find the names of the top 5 genes
top_5_genes <- names(gene_scores_ranked[1:5])
top_5_genes
# Show the score of each top 5 genes
pca$rotation[top_5_genes, 1]
```

The way to interpret these gene scores is, the genes with the most positive loading scores are the ones that pushed the ko samples to the positive (right) side of the plot. Conversely, the genes with the most negative loading scores are the ones that pushed the wt samples to the negative (left) side of the plot.

